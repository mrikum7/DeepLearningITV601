{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/mwpl/anaconda3/envs/tf_gpu/lib/python3.8/site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: h5py in /home/mwpl/anaconda3/envs/tf_gpu/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/mwpl/anaconda3/envs/tf_gpu/lib/python3.8/site-packages (from keras) (1.6.1)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "Requirement already satisfied: six in /home/mwpl/anaconda3/envs/tf_gpu/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: pyyaml, keras\n",
      "Successfully installed keras-2.4.3 pyyaml-5.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.color import rgb2gray\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.list_physical_devices('GPU')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for j in range(9):\n",
    "    for i, file in enumerate(os.listdir('../Datasets/xPLNET/DATASET/Training Samples/' + str(j))):\n",
    "        data = Image.open(f'../Datasets/xPLNET/DATASET/Training Samples/{j}/' + file)\n",
    "        data = np.asarray(data)\n",
    "        X_train.append(data)\n",
    "    y_train.extend([j for n in range(i + 1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59184, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((59184, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59184, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation = 'relu',\n",
    "                              input_shape = (64, 64, 3)))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "#model.add(keras.layers.Dropout(0.5, input_shape = (100, )))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(9, activation = 'softmax'))\n",
    "model.compile(optimizer = keras.optimizers.Adam(),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_1\", activity_regularizer=None, trainable=True, input_dtype=\"float32\", batch_input_shape=[None, 3, ..., activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(gamma_regularizer=None, name=\"batchnormalization_1\", epsilon=1e-05, trainable=True, beta_regularizer=None, momentum=0.99, axis=-1)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_2\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_1\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(gamma_regularizer=None, name=\"batchnormalization_2\", epsilon=1e-05, trainable=True, beta_regularizer=None, momentum=0.99, axis=-1)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_3\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_2\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(gamma_regularizer=None, name=\"batchnormalization_3\", epsilon=1e-05, trainable=True, beta_regularizer=None, momentum=0.99, axis=-1)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_4\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_3\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(gamma_regularizer=None, name=\"batchnormalization_4\", epsilon=1e-05, trainable=True, beta_regularizer=None, momentum=0.99, axis=-1)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(name=\"convolution2d_5\", activity_regularizer=None, trainable=True, activation=\"relu\", kernel_size=(3, 3), filters=128, strides=[1, 1], padding=\"valid\", data_format=\"channels_first\", kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(name=\"maxpooling2d_4\", trainable=True, pool_size=[2, 2], strides=[2, 2], padding=\"valid\", data_format=\"channels_first\")`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(trainable=True, name=\"dropout_1\", rate=0.5)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"dense_1\", activity_regularizer=None, trainable=True, input_dim=512, activation=\"relu\", units=500, kernel_initializer=\"normal\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(trainable=True, name=\"dropout_2\", rate=0.5)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"dense_2\", activity_regularizer=None, trainable=True, input_dim=500, activation=\"relu\", units=100, kernel_initializer=\"normal\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(trainable=True, name=\"dropout_3\", rate=0.5)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py:1179: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(name=\"dense_3\", activity_regularizer=None, trainable=True, input_dim=100, activation=\"softmax\", units=9, kernel_initializer=\"normal\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/mrikum7/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../Datasets/xPLNET/model_weights/weights_arch1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convolution2d_1 (Conv2D)     (None, 128, 62, 62)       3584      \n",
      "_________________________________________________________________\n",
      "batchnormalization_1 (BatchN (None, 128, 62, 62)       248       \n",
      "_________________________________________________________________\n",
      "convolution2d_2 (Conv2D)     (None, 128, 60, 60)       147584    \n",
      "_________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D (None, 128, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "batchnormalization_2 (BatchN (None, 128, 30, 30)       120       \n",
      "_________________________________________________________________\n",
      "convolution2d_3 (Conv2D)     (None, 128, 28, 28)       147584    \n",
      "_________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D (None, 128, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "batchnormalization_3 (BatchN (None, 128, 14, 14)       56        \n",
      "_________________________________________________________________\n",
      "convolution2d_4 (Conv2D)     (None, 128, 12, 12)       147584    \n",
      "_________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D (None, 128, 6, 6)         0         \n",
      "_________________________________________________________________\n",
      "batchnormalization_4 (BatchN (None, 128, 6, 6)         24        \n",
      "_________________________________________________________________\n",
      "convolution2d_5 (Conv2D)     (None, 128, 4, 4)         147584    \n",
      "_________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D (None, 128, 2, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 901,877\n",
      "Trainable params: 901,653\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 5504/59184 [=>............................] - ETA: 9:04 - loss: 5.9658 - accuracy: 0.1524"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1057686c6151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3748\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3749\u001b[0m         expand_composites=True)\n\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3746\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3748\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3749\u001b[0m         expand_composites=True)\n\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for j in range(9):\n",
    "    for i, file in enumerate(os.listdir('../Datasets/xPLNET/DATASET/Test Samples/' + str(j))):\n",
    "        data = Image.open(f'../Datasets/xPLNET/DATASET/Test Samples/{j}/' + file)\n",
    "        data = np.asarray(data)\n",
    "        X_test.append(data)\n",
    "    y_test.extend([j for n in range(i + 1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6576, 64, 64, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = x_test.reshape((6576, 64, 64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 4s 18ms/step - loss: 0.7920 - accuracy: 0.7641\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mwpl/DeepLearningITV601/Datasets/xPLNET\n"
     ]
    }
   ],
   "source": [
    "cd Datasets/xPLNET/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mDATASET\u001b[0m/  \u001b[01;34mmodel_weights\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mwpl/DeepLearningITV601/Datasets/xPLNET/model_weights\n"
     ]
    }
   ],
   "source": [
    "cd model_weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt~  weights_arch1.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 12 layers into a model with 0 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-7410fe77ee27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights_arch1.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1265\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    680\u001b[0m   \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m     raise ValueError('You are trying to load a weight file '\n\u001b[0m\u001b[1;32m    683\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 12 layers into a model with 0 layers."
     ]
    }
   ],
   "source": [
    "model.load_weights('weights_arch1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape = (64, 64, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DHA8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submit this DHA by Sunday 2:00 PM \n",
    "\n",
    "## 1. Create your own Convnet\n",
    "## 2. Create three classes of Random shapes (triangle, square, circle), 1000 each (100, 100)\n",
    "## 3. Testing Data (100, 100, 100)\n",
    "## 4. Train your CNN on training Data\n",
    "## 5. Accuracy on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
